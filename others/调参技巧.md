
（1）relu+bn。这套好基友组合是万精油，可以满足95%的情况，除非有些特殊情况会用identity，比如回归问题，比如resnet的shortcut支路

（2）dropout 。分类问题用dropout ，只需要最后一层softmax 前用基本就可以了，能够防止过拟合，可能对accuracy提高不大，但是dropout 前面的那层如果是之后要使用的feature的话，性能会大大提升

（3）数据的shuffle 和augmentation 。这个没啥好说的，aug也不是瞎加，比如行人识别一般就不会加上下翻转的，因为不会碰到头朝下的异型种

（4）降学习率。随着网络训练的进行，学习率要逐渐降下来，如果你有tensorboard，你有可能发现，在学习率下降的一瞬间，网络会有个巨大的性能提升，同样的fine-tuning也要根据模型的性能设置合适的学习率，比如一个训练的已经非常好的模型你上来就1e-3的学习率，那之前就白训练了，就是说网络性能越好，学习率要越小

（5）tensorboard。以前不怎么用，用了之后发现太有帮助，帮助你监视网络的状态，来调整网络参数

（6）随时存档模型，要有validation 。这就跟打游戏一样存档，把每个epoch和其对应的validation 结果存下来，可以分析出开始overfitting的时间点，方便下次加载fine-tuning

（7）网络层数，参数量什么的都不是大问题，在性能不丢的情况下，减到最小

（8）batchsize通常影响没那么大，塞满卡就行，除了特殊的算法需要batch大一点

（9）输入减不减mean归一化在有了bn之后已经不那么重要了

上面那些都是大家所知道的常识，也是外行人觉得深度学习一直在做的就是这些很low的东西，其实网络设计上博大精深，这也远超过我的水平范畴，只说一些很简单的

（1）卷积核的分解。从最初的5×5分解为两个3×3，到后来的3×3分解为1×3和3×1，再到resnet的1×1，3×3，1×1，再xception的3×3 channel-wise conv+1×1，网络的计算量越来越小，层数越来越多，性能越来越好，这些都是设计网络时可以借鉴的

（2）不同尺寸的feature maps的concat，只用一层的feature map一把梭可能不如concat好，pspnet就是这种思想，这个思想很常用

（3）resnet的shortcut确实会很有用，重点在于shortcut支路一定要是identity，主路是什么conv都无所谓，这是我亲耳听resnet作者所述

（4）针对于metric learning，对feature加个classification 的约束通常可以提高性能加快收敛
